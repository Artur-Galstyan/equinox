{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab229dca-2c2a-46ee-8f24-eedce5c06e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0591c0a-21c6-4ff4-b99f-4628dcc076df",
   "metadata": {},
   "source": [
    "# Mamba\n",
    "\n",
    "In this example, we will implement the new Mamba model from Albert Gu and Tri Dao [[1]](https://arxiv.org/abs/2312.00752) by utilising the new `SelectiveStateSpaceModel` layer. \n",
    "\n",
    "In this example, you will learn the following:\n",
    "\n",
    "    - how to implement Mamba\n",
    "    - how to use a shared layer\n",
    "\n",
    "Special thanks and cretits go to John (Zhiyao) Ma and his excellent Mamba implementation in PyTorch, which served as a great inspriration and foundation for this Equinox version. Go check it out [here](https://github.com/johnma2006/mamba-minimal).\n",
    "\n",
    "The original implementation includes **a lot** of CUDA code [[2]](https://github.com/state-spaces/mamba) to optimise the so-called `selective_scan` algorithm, but this first iteration of the `SelectiveStateSpaceModel` implementation is not as heavily optimised. However, in future iterations, by using some clever Pallas code, we can get to the same performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418d2b9-80f8-4048-9b8b-d8e958035705",
   "metadata": {},
   "source": [
    "The following image shows the high level architecture of Mamba which we will implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660315df-3e77-4e3c-a793-c153840f094c",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; margin-left: auto; width: 100%\">\n",
    "    <img src=\"../imgs/Mamba1.drawio.svg\" width=\"30%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b1134-b021-4fae-97d7-90c6afb66cd5",
   "metadata": {},
   "source": [
    "If we zoom into the `ResidualBlock`, we find the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b5de1-8300-4fc4-934e-3b0194bf8372",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; margin-left: auto; width: 100%\">\n",
    "    <img src=\"../imgs/Mamba2.drawio.svg\" width=\"30%\">\n",
    "</div>\n",
    "\n",
    "As you can see, we keep diving further into the model. Let's implement this `ResidualBlock` now. Let's keep on zooming until we get to the deepest component - at which point we can start to implement everything and work our way back up. Let's keep going."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8562a8b-3bea-4997-8c53-be33d66893f1",
   "metadata": {},
   "source": [
    "We're getting closer and closer to the heart of the Mamba model. Let's look at what the `MambaBlock` looks like. This time, I've included the shapes of the matrices as they traverse through all kinds of transformations. \n",
    "\n",
    "<div style=\"display: flex; justify-content: center; margin-left: auto; width: 100%\">\n",
    "    <img src=\"../imgs/Mamba3.drawio.svg\" width=\"60%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c528d-2f7a-4696-808e-f1fe9db2110d",
   "metadata": {},
   "source": [
    "Most of the parts we need are already present in Equinox's library. What's missing though is the new `SelectiveStateSpaceModel` (abbreviated as `SSM` above). Everything in green are trainable parameters. \n",
    "\n",
    "<div style=\"display: flex; justify-content: center; margin-left: auto; width: 100%\">\n",
    "    <img src=\"../imgs/Mamba4.drawio.svg\" width=\"60%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88cd1b-eb91-4af1-b537-f209342a8bb1",
   "metadata": {},
   "source": [
    "Alright! This is the deepest we can get. We've reached the point at which we have all needed components available to us (except for the `selective_scan` function, but that's not a problem). Let's start with the `SelectiveStateSpaceModel` and then work our way back up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efd5640-cdb6-427d-be3d-860cf371e357",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Float' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselective_scan\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     x: \u001b[43mFloat\u001b[49m[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length d_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     delta: Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length d_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m     A: Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_inner d_state\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     B: Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length d_state\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     C: Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length d_state\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     D: Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m d_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length d_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      9\u001b[0m     L, d_inner \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     10\u001b[0m     _, d_state \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Float' is not defined"
     ]
    }
   ],
   "source": [
    "def selective_scan(\n",
    "    x: Float[Array, \"seq_length d_inner\"],\n",
    "    delta: Float[Array, \"seq_length d_inner\"],\n",
    "    A: Float[Array, \"d_inner d_state\"],\n",
    "    B: Float[Array, \"seq_length d_state\"],\n",
    "    C: Float[Array, \"seq_length d_state\"],\n",
    "    D: Float[Array, \" d_inner\"],\n",
    ") -> Float[Array, \"seq_length d_inner\"]:\n",
    "    L, d_inner = x.shape\n",
    "    _, d_state = A.shape\n",
    "    delta_A = jnp.exp(jnp.einsum(\"l d,d n -> l d n\", delta, A))\n",
    "    delta_B_u = jnp.einsum(\"l d,l n,l d -> l d n\", delta, B, x)\n",
    "\n",
    "    x_res = jnp.zeros(shape=(d_inner, d_state))\n",
    "\n",
    "    def step(x, i):\n",
    "        x = delta_A[i] * x + delta_B_u[i]\n",
    "\n",
    "        y = jnp.einsum(\"d n,n -> d\", x, C[i, :])\n",
    "        return x, y\n",
    "\n",
    "    _, ys = jax.lax.scan(step, x_res, jnp.arange(L))\n",
    "\n",
    "    ys = ys + x * D\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0856563-ef82-4deb-91cf-f34f6c8793bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveStateSpaceModel(eqx.Module, strict=True):\n",
    "    input_proj: eqx.nn.Linear\n",
    "    delta_proj: eqx.nn.Linear\n",
    "    A_log: Float[Array, \"d_inner d_state\"]\n",
    "    D: Float[Array, \" d_inner\"]\n",
    "\n",
    "    d_inner: int = eqx.field(static=True)\n",
    "    dt_rank: int = eqx.field(static=True)\n",
    "    d_state: int = eqx.field(static=True)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_state: int,\n",
    "        use_input_proj_bias: bool = False,\n",
    "        use_delta_proj_bias: bool = False,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        self.d_inner = d_inner\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        (\n",
    "            key,\n",
    "            input_proj_key,\n",
    "            delta_proj_key,\n",
    "        ) = jax.random.split(key, 3)\n",
    "        self.input_proj = eqx.nn.Linear(\n",
    "            d_inner,\n",
    "            dt_rank + d_state * 2,\n",
    "            use_bias=use_input_proj_bias,\n",
    "            key=input_proj_key,\n",
    "        )\n",
    "\n",
    "        self.delta_proj = eqx.nn.Linear(\n",
    "            dt_rank, d_inner, use_bias=use_delta_proj_bias, key=delta_proj_key\n",
    "        )\n",
    "        A = jnp.repeat(jnp.arange(1, d_state + 1), d_inner).reshape(d_inner, d_state)\n",
    "        self.A_log = jnp.log(A)\n",
    "        self.D = jnp.ones(d_inner)\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"seq_length d_inner\"]):\n",
    "        A = -jnp.exp(self.A_log)\n",
    "        D = self.D\n",
    "\n",
    "        delta_b_c = jax.vmap(self.input_proj)(x)\n",
    "\n",
    "        split_indices = [\n",
    "            self.dt_rank,\n",
    "            self.dt_rank + self.d_state,\n",
    "        ]\n",
    "        delta, B, C = jnp.split(delta_b_c, split_indices, axis=-1)\n",
    "        delta = jax.nn.softplus(jax.vmap(self.delta_proj)(delta))\n",
    "\n",
    "        y = selective_scan(x, delta, A, B, C, D)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44075c-e70c-4d93-8f6b-1b160cc7e7dd",
   "metadata": {},
   "source": [
    "## Detour: State Space Models\n",
    "___TODO___: Explain SSMs in general!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48addc-b78f-429b-9299-d1e283f3bd76",
   "metadata": {},
   "source": [
    "Armed with the `SSM`, we can now implement the `MambaBlock` part. See the images above for where we are right now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57d346-18d0-4d29-b35d-e1a6e99c6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(eqx.Module):\n",
    "    in_proj: eqx.nn.Linear\n",
    "    conv1d: eqx.nn.Conv1d\n",
    "    ssm: SelectiveStateSpaceModel\n",
    "    out_proj: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd: int,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_conv: int,\n",
    "        use_in_projection_bias: bool = True,\n",
    "        use_conv_bias: bool = True,\n",
    "        use_out_proj_bias: bool = True,\n",
    "        ssm_use_delta_proj_bias: bool = False,\n",
    "        ssm_use_input_proj_bias: bool = False,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        (\n",
    "            key,\n",
    "            linear_key,\n",
    "            conv1d_key,\n",
    "            ssm_key,\n",
    "            out_proj_key,\n",
    "        ) = jax.random.split(key, 5)\n",
    "\n",
    "        self.in_proj = eqx.nn.Linear(\n",
    "            n_embd,\n",
    "            d_inner * 2,\n",
    "            use_bias=use_in_projection_bias,\n",
    "            key=linear_key,\n",
    "        )\n",
    "\n",
    "        self.conv1d = eqx.nn.Conv1d(\n",
    "            in_channels=d_inner,\n",
    "            out_channels=d_inner,\n",
    "            kernel_size=d_conv,\n",
    "            use_bias=use_conv_bias,\n",
    "            groups=d_inner,\n",
    "            padding=d_conv - 1,\n",
    "            key=conv1d_key,\n",
    "        )\n",
    "        self.ssm = SelectiveStateSpaceModel(\n",
    "            d_inner=d_inner,\n",
    "            dt_rank=dt_rank,\n",
    "            d_state=d_inner,\n",
    "            use_delta_proj_bias=ssm_use_delta_proj_bias,\n",
    "            use_input_proj_bias=ssm_use_input_proj_bias,\n",
    "            key=ssm_key,\n",
    "        )\n",
    "        self.out_proj = eqx.nn.Linear(\n",
    "            d_inner,\n",
    "            n_embd,\n",
    "            use_bias=use_out_proj_bias,\n",
    "            key=out_proj_key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: Array):\n",
    "        seq_len, d = x.shape\n",
    "        x_and_res = jax.vmap(self.in_proj)(x)\n",
    "\n",
    "        (x, res) = jnp.split(x_and_res, 2, axis=-1)\n",
    "        x = jnp.transpose(x)\n",
    "        x = self.conv1d(x)[:, :seq_len]\n",
    "        x = jnp.transpose(x)\n",
    "        x = jax.nn.silu(x)\n",
    "\n",
    "        y = self.ssm(x)\n",
    "        y = y * jax.nn.silu(res)\n",
    "\n",
    "        output = jax.vmap(self.out_proj)(y)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7543f-5bc9-480e-bab4-7f839cc8faad",
   "metadata": {},
   "source": [
    "Now, we can wrap the `MambaBlock` into the `ResidualBlock` -- as the name suggests, this has a residual connection (or in non-_sciency_ words: it adds the original input to the transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed52a9-abea-43e8-822a-9d75ac8ae480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(eqx.Module, strict=True):\n",
    "    mamba_block: MambaBlock\n",
    "    rns_norm: eqx.nn.RMSNorm\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd: int,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_conv: int,\n",
    "        use_in_projection_bias: bool = True,\n",
    "        use_conv_bias: bool = True,\n",
    "        use_out_proj_bias: bool = True,\n",
    "        ssm_use_delta_proj_bias: bool = False,\n",
    "        ssm_use_input_proj_bias: bool = False,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        self.mamba_block = MambaBlock(\n",
    "            n_embd=n_embd,\n",
    "            d_inner=d_inner,\n",
    "            dt_rank=dt_rank,\n",
    "            d_conv=d_conv,\n",
    "            use_in_projection_bias=use_in_projection_bias,\n",
    "            use_conv_bias=use_conv_bias,\n",
    "            use_out_proj_bias=use_out_proj_bias,\n",
    "            ssm_use_delta_proj_bias=ssm_use_delta_proj_bias,\n",
    "            ssm_use_input_proj_bias=ssm_use_input_proj_bias,\n",
    "            key=key,\n",
    "        )\n",
    "        self.rns_norm = eqx.nn.RMSNorm(n_embd)\n",
    "\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"seq_len n_embd\"], *, key: Optional[PRNGKeyArray] = None\n",
    "    ) -> Array:\n",
    "        return self.mamba_block(jax.vmap(self.rns_norm)(x)) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8ed93-4946-4758-a228-cc39326b8c9d",
   "metadata": {},
   "source": [
    "We've arrived at the highest point again. We can put everything into the `Mamba` class now. Note that the weights of the embedding layer and the final linear layer are shared! This is not a problem though, because we can use `eqx.nn.Shared` to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91463596-114f-45f7-a2f0-b1ae8d16f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(eqx.Module, strict=True):\n",
    "    layers: eqx.nn.Sequential\n",
    "    normalization: eqx.nn.RMSNorm\n",
    "    shared_emb_lm_head: eqx.nn.Shared\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int,\n",
    "        n_dims: int,\n",
    "        n_embd: int,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_conv: int,\n",
    "        use_in_projection_bias: bool = True,\n",
    "        use_conv_bias: bool = True,\n",
    "        use_out_proj_bias: bool = True,\n",
    "        ssm_use_delta_proj_bias: bool = False,\n",
    "        ssm_use_input_proj_bias: bool = False,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        key, *subkeys = jax.random.split(key, 1 + n_layers)\n",
    "        self.layers = eqx.nn.Sequential(\n",
    "            [\n",
    "                ResidualBlock(\n",
    "                    n_embd=n_embd,\n",
    "                    d_inner=d_inner,\n",
    "                    dt_rank=dt_rank,\n",
    "                    d_conv=d_conv,\n",
    "                    use_in_projection_bias=use_in_projection_bias,\n",
    "                    use_conv_bias=use_conv_bias,\n",
    "                    use_out_proj_bias=use_out_proj_bias,\n",
    "                    ssm_use_delta_proj_bias=ssm_use_delta_proj_bias,\n",
    "                    ssm_use_input_proj_bias=ssm_use_input_proj_bias,\n",
    "                    key=subkeys[i + 1],\n",
    "                )\n",
    "                for i in range(n_layers)\n",
    "            ],\n",
    "        )\n",
    "        self.normalization = eqx.nn.RMSNorm(n_embd)\n",
    "\n",
    "        embedding = eqx.nn.Embedding(n_dims, n_embd, key=subkeys[0])\n",
    "        lm_head = eqx.nn.Linear(\n",
    "            n_embd,\n",
    "            n_dims,\n",
    "            use_bias=False,\n",
    "            key=subkeys[-1],\n",
    "        )\n",
    "        where = lambda embed_and_lin: embed_and_lin[1].weight\n",
    "        get = lambda embed_and_lin: embed_and_lin[0].weight\n",
    "        self.shared_emb_lm_head = eqx.nn.Shared(\n",
    "            (embedding, lm_head), where=where, get=get\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: Int[Array, \"seq_len\"],  # noqa\n",
    "        *,\n",
    "        key: Optional[PRNGKeyArray] = None,\n",
    "    ) -> Float[Array, \"seq_len n_dims\"]:  # noqa\n",
    "        embedding, linear = self.shared_emb_lm_head()\n",
    "        x = jax.vmap(embedding)(x)\n",
    "\n",
    "        x = self.layers(x)\n",
    "        x = jax.vmap(self.normalization)(x)\n",
    "        logits = jax.vmap(linear)(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557fa4a-1fa4-4e72-ba77-d0ef7f946cdf",
   "metadata": {},
   "source": [
    "Note the usage of `eqx.nn.Shared`:\n",
    "\n",
    "```python\n",
    "    # Embedding layer\n",
    "    embedding = eqx.nn.Embedding(\n",
    "        n_dims, n_embd, key=subkeys[0]\n",
    "    )\n",
    "    # Linear layer\n",
    "    lm_head = eqx.nn.Linear(\n",
    "        n_embd,\n",
    "        n_dims,\n",
    "        use_bias=False,\n",
    "        key=subkeys[-1],\n",
    "    )\n",
    "    # refers to the linear weights\n",
    "    where = lambda embed_and_lin: embed_and_lin[1].weight \n",
    "\n",
    "    # refers to the embedding weights\n",
    "    get = lambda embed_and_lin: embed_and_lin[0].weight\n",
    "\n",
    "    # Create a shared layer\n",
    "    self.shared_emb_lm_head = eqx.nn.Shared(\n",
    "        (embedding, lm_head), where=where, get=get\n",
    "    )\n",
    "```\n",
    "\n",
    "And to use the shared layers, we have to get them first out of the shared layer:\n",
    "\n",
    "```python\n",
    "    embedding, linear = self.shared_emb_lm_head()\n",
    "    # embedding and linear are eqx.nn.Embedding and eqx.nn.Linear respectively\n",
    "    # proceed usage as usual\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07491e-70ea-43b3-9139-0940caf1dd50",
   "metadata": {},
   "source": [
    "Excellent! We have successfully implemented the Mamba model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96aabdf-081a-42b7-8784-2e1809f7562a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
