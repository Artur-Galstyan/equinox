{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0591c0a-21c6-4ff4-b99f-4628dcc076df",
   "metadata": {},
   "source": [
    "# Mamba\n",
    "\n",
    "In this example, we will implement the new Mamba model from Albert Gu and Tri Dao [[1]](https://arxiv.org/abs/2312.00752) by utilising the new `SelectiveStateSpaceModel` layer. \n",
    "\n",
    "In this example, you will learn the following:\n",
    "\n",
    "    - how to implement Mamba\n",
    "    - how to use a shared layer\n",
    "\n",
    "Special thanks and cretits go to John (Zhiyao) Ma and his excellent Mamba implementation in PyTorch, which served as a great inspriration and foundation for this Equinox version. Go check it out [here](https://github.com/johnma2006/mamba-minimal).\n",
    "\n",
    "The original implementation includes **a lot** of CUDA code [[2]](https://github.com/state-spaces/mamba) to optimise the so-called `selective_scan` algorithm, but this first iteration of the `SelectiveStateSpaceModel` implementation is not as heavily optimised. However, in future iterations, by using some clever Pallas code, we can get to the same performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418d2b9-80f8-4048-9b8b-d8e958035705",
   "metadata": {},
   "source": [
    "The following image shows the high level architecture of Mamba which we will implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660315df-3e77-4e3c-a793-c153840f094c",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; margin-left: auto; width: 100%\">\n",
    "    <img src=\"../imgs/Mamba1.drawio.svg\" width=\"30%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8ed93-4946-4758-a228-cc39326b8c9d",
   "metadata": {},
   "source": [
    "Before dive into the `ResidualBlock` part, which contains the main `SelectiveStateSpaceModel` code, let's quickly build everything around it first. Also note that the weights of the embedding layer and the final linear layer are shared! This is not a problem though, because we can use `eqx.nn.Shared` to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab229dca-2c2a-46ee-8f24-eedce5c06e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91463596-114f-45f7-a2f0-b1ae8d16f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(eqx.Module, strict=True):\n",
    "    layers: eqx.nn.Sequential\n",
    "    normalization: eqx.nn.RMSNorm\n",
    "    shared_emb_lm_head: eqx.nn.Shared\n",
    "\n",
    "    def __init__(self, n_layers: int, n_dims: int, n_embd: int, *, key: PRNGKeyArray):\n",
    "        key, *subkeys = jax.random.split(key, 1 + n_layers)\n",
    "        self.layers = eqx.nn.Sequential(\n",
    "            [ResidualBlock(key=subkeys[i + 1]) for i in range(n_layers)],\n",
    "        )\n",
    "        self.normalization = eqx.nn.RMSNorm(n_embd)\n",
    "\n",
    "        embedding = eqx.nn.Embedding(n_dims, n_embd, key=subkeys[0])\n",
    "        lm_head = eqx.nn.Linear(\n",
    "            n_embd,\n",
    "            n_dims,\n",
    "            use_bias=False,\n",
    "            key=subkeys[-1],\n",
    "        )\n",
    "        where = lambda embed_and_lin: embed_and_lin[1].weight\n",
    "        get = lambda embed_and_lin: embed_and_lin[0].weight\n",
    "        self.shared_emb_lm_head = eqx.nn.Shared(\n",
    "            (embedding, lm_head), where=where, get=get\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: Int[Array, \"seq_len\"],  # noqa\n",
    "        *,\n",
    "        key: Optional[PRNGKeyArray] = None,\n",
    "    ) -> Float[Array, \"seq_len n_dims\"]:  # noqa\n",
    "        embedding, linear = self.shared_emb_lm_head()\n",
    "        x = jax.vmap(embedding)(x)\n",
    "\n",
    "        x = self.layers(x)\n",
    "        x = jax.vmap(self.normalization)(x)\n",
    "        logits = jax.vmap(linear)(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557fa4a-1fa4-4e72-ba77-d0ef7f946cdf",
   "metadata": {},
   "source": [
    "We haven't implementated `ResidualBlock` yet, but we will get there soon. Note the usage of `eqx.nn.Shared`:\n",
    "\n",
    "```python\n",
    "    # Embedding layer\n",
    "    embedding = eqx.nn.Embedding(\n",
    "        n_dims, n_embd, key=subkeys[0]\n",
    "    )\n",
    "    # Linear layer\n",
    "    lm_head = eqx.nn.Linear(\n",
    "        n_embd,\n",
    "        n_dims,\n",
    "        use_bias=False,\n",
    "        key=subkeys[-1],\n",
    "    )\n",
    "    # refers to the linear weights\n",
    "    where = lambda embed_and_lin: embed_and_lin[1].weight \n",
    "\n",
    "    # refers to the embedding weights\n",
    "    get = lambda embed_and_lin: embed_and_lin[0].weight\n",
    "\n",
    "    # Create a shared layer\n",
    "    self.shared_emb_lm_head = eqx.nn.Shared(\n",
    "        (embedding, lm_head), where=where, get=get\n",
    "    )\n",
    "```\n",
    "\n",
    "And to use the shared layers, we have to get them first out of the shared layer:\n",
    "\n",
    "```python\n",
    "    embedding, linear = self.shared_emb_lm_head()\n",
    "    # embedding and linear are eqx.nn.Embedding and eqx.nn.Linear respectively\n",
    "    # proceed usage as usual\n",
    "```\n",
    "\n",
    "Let's continue with the `ResidualBlock`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b5de1-8300-4fc4-934e-3b0194bf8372",
   "metadata": {},
   "source": [
    "Here's an overview of what the components of the `ResidualBlock` will look like.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; margin-left: auto; width: 100%\">\n",
    "    <img src=\"../imgs/Mamba2.drawio.svg\" width=\"30%\">\n",
    "</div>\n",
    "\n",
    "As you can see, we keep diving further into the model. Let's implement this `ResidualBlock` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ed52a9-abea-43e8-822a-9d75ac8ae480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(eqx.Module, strict=True):\n",
    "    mamba_block: MambaBlock\n",
    "    rns_norm: eqx.nn.RMSNorm\n",
    "\n",
    "    def __init__(self, n_embd: int, *, key: PRNGKeyArray):\n",
    "        self.mamba_block = MambaBlock(\n",
    "            key=key,\n",
    "        )\n",
    "        self.rns_norm = eqx.nn.RMSNorm(n_embd)\n",
    "\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"seq_len n_embd\"], *, key: Optional[PRNGKeyArray] = None\n",
    "    ) -> Array:\n",
    "        return self.mamba_block(jax.vmap(self.rns_norm)(x)) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8562a8b-3bea-4997-8c53-be33d66893f1",
   "metadata": {},
   "source": [
    "We're getting closer and closer to the heart of the Mamba model. Let's look at what the `MambaBlock` looks like. This time, I've included the shapes of the matrices as they traverse through all kinds of transformations. \n",
    "\n",
    "<div style=\"display: flex; justify-content: center; margin-left: auto; width: 100%\">\n",
    "    <img src=\"../imgs/Mamba3.drawio.svg\" width=\"60%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b57d346-18d0-4d29-b35d-e1a6e99c6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(eqx.Module):\n",
    "    in_proj: eqx.nn.Linear\n",
    "    conv1d: eqx.nn.Conv1d\n",
    "    ssm: SSM\n",
    "    out_proj: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd: int,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_conv: int,\n",
    "        use_in_projection_bias: bool=True, \n",
    "        use_conv_bias: bool=True,\n",
    "        use_out_proj_bias: bool = True,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        (\n",
    "            key,\n",
    "            linear_key,\n",
    "            conv1d_key,\n",
    "            ssm_key,\n",
    "            out_proj_key,\n",
    "        ) = jax.random.split(key, 5)\n",
    "\n",
    "        self.in_proj = eqx.nn.Linear(\n",
    "            n_embd,\n",
    "            d_inner * 2,\n",
    "            use_bias=use_in_projection_bias,\n",
    "            key=linear_key,\n",
    "        )\n",
    "\n",
    "        self.conv1d = eqx.nn.Conv1d(\n",
    "            in_channels=d_inner,\n",
    "            out_channels=d_inner,\n",
    "            kernel_size=d_conv,\n",
    "            use_bias=use_conv_bias,\n",
    "            groups=d_inner,\n",
    "            padding=d_conv - 1,\n",
    "            key=conv1d_key,\n",
    "        )\n",
    "        self.ssm = SSM(key=ssm_key)\n",
    "        self.out_proj = eqx.nn.Linear(\n",
    "            d_inner,\n",
    "            n_embd,\n",
    "            use_bias=use_out_proj_bias,\n",
    "            key=out_proj_key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: Array):\n",
    "        seq_len, d = x.shape\n",
    "        x_and_res = jax.vmap(self.in_proj)(x)\n",
    "\n",
    "        (x, res) = jnp.split(x_and_res, 2, axis=-1)\n",
    "        x = jnp.transpose(x)\n",
    "        x = self.conv1d(x)[:, :seq_len]\n",
    "        x = jnp.transpose(x)\n",
    "        x = jax.nn.silu(x)\n",
    "\n",
    "        y = self.ssm(x)\n",
    "        y = y * jax.nn.silu(res)\n",
    "\n",
    "        output = jax.vmap(self.out_proj)(y)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84076b2-9808-4916-8580-d789cdc49c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSM(eqx.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196448d-c659-44d2-9527-6b29723b2c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
