{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab229dca-2c2a-46ee-8f24-eedce5c06e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray, PyTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0591c0a-21c6-4ff4-b99f-4628dcc076df",
   "metadata": {},
   "source": [
    "# Mamba\n",
    "\n",
    "In this example, we will implement the new Mamba model from Albert Gu and Tri Dao [[1]](https://arxiv.org/abs/2312.00752) by utilising the new `SelectiveStateSpaceModel` layer. \n",
    "\n",
    "In this example, you will learn the following:\n",
    "\n",
    "    - how to implement Mamba\n",
    "    - how to use a shared layer\n",
    "\n",
    "Special thanks and cretits go to John (Zhiyao) Ma and his excellent Mamba implementation in PyTorch, which served as a great inspriration and foundation for this Equinox version. Go check it out [here](https://github.com/johnma2006/mamba-minimal).\n",
    "\n",
    "Author: [Artur Galstyan](https://github.com/artur-galstyan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1861d8-d012-46d0-9a72-23d940674d2d",
   "metadata": {},
   "source": [
    "The original implementation includes **a lot** of CUDA code [[2]](https://github.com/state-spaces/mamba) to optimise the so-called `selective_scan` algorithm, but this first iteration of the `SelectiveStateSpaceModel` implementation is not as heavily optimised. However, in future iterations, by using some clever Pallas code, we can get to the same performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418d2b9-80f8-4048-9b8b-d8e958035705",
   "metadata": {},
   "source": [
    "The following image shows the high level architecture of Mamba which we will implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660315df-3e77-4e3c-a793-c153840f094c",
   "metadata": {},
   "source": [
    "![Mamba](../imgs/Mamba1.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b1134-b021-4fae-97d7-90c6afb66cd5",
   "metadata": {},
   "source": [
    "If we zoom into the `ResidualBlock`, we find the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b5de1-8300-4fc4-934e-3b0194bf8372",
   "metadata": {},
   "source": [
    "![Mamba](../imgs/Mamba2.drawio.svg)\n",
    "\n",
    "As you can see, we keep diving further into the model. Let's implement this `ResidualBlock` now. Let's keep on zooming until we get to the deepest component - at which point we can start to implement everything and work our way back up. Let's keep going."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8562a8b-3bea-4997-8c53-be33d66893f1",
   "metadata": {},
   "source": [
    "We're getting closer and closer to the heart of the Mamba model. Let's look at what the `MambaBlock` looks like. This time, I've included the shapes of the matrices as they traverse through all kinds of transformations. \n",
    "\n",
    "![Mamba](../imgs/Mamba3.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c528d-2f7a-4696-808e-f1fe9db2110d",
   "metadata": {},
   "source": [
    "Most of the parts we need are already present in Equinox's library. What's missing though is the new `SelectiveStateSpaceModel` (abbreviated as `SSM` above). Everything in green are trainable parameters. \n",
    "\n",
    "![Mamba](../imgs/Mamba4.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88cd1b-eb91-4af1-b537-f209342a8bb1",
   "metadata": {},
   "source": [
    "Alright! This is the deepest we can get. We've reached the point at which we have all needed components available to us (except for the `selective_scan` function, but that's not a problem). Let's start with the `SelectiveStateSpaceModel` and then work our way back up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efd5640-cdb6-427d-be3d-860cf371e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_scan(\n",
    "    x: Float[Array, \"seq_length d_inner\"],\n",
    "    delta: Float[Array, \"seq_length d_inner\"],\n",
    "    A: Float[Array, \"d_inner d_state\"],\n",
    "    B: Float[Array, \"seq_length d_state\"],\n",
    "    C: Float[Array, \"seq_length d_state\"],\n",
    "    D: Float[Array, \" d_inner\"],\n",
    ") -> Float[Array, \"seq_length d_inner\"]:\n",
    "    L, d_inner = x.shape\n",
    "    _, d_state = A.shape\n",
    "    delta_A = jnp.exp(jnp.einsum(\"l d,d n -> l d n\", delta, A))\n",
    "    delta_B_u = jnp.einsum(\"l d,l n,l d -> l d n\", delta, B, x)\n",
    "\n",
    "    x_res = jnp.zeros(shape=(d_inner, d_state))\n",
    "\n",
    "    def step(x, i):\n",
    "        x = delta_A[i] * x + delta_B_u[i]\n",
    "\n",
    "        y = jnp.einsum(\"d n,n -> d\", x, C[i, :])\n",
    "        return x, y\n",
    "\n",
    "    _, ys = jax.lax.scan(step, x_res, jnp.arange(L))\n",
    "\n",
    "    ys = ys + x * D\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0856563-ef82-4deb-91cf-f34f6c8793bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveStateSpaceModel(eqx.Module, strict=True):\n",
    "    input_proj: eqx.nn.Linear\n",
    "    delta_proj: eqx.nn.Linear\n",
    "    A_log: Float[Array, \"d_inner d_state\"]\n",
    "    D: Float[Array, \" d_inner\"]\n",
    "\n",
    "    d_inner: int = eqx.field(static=True)\n",
    "    dt_rank: int = eqx.field(static=True)\n",
    "    d_state: int = eqx.field(static=True)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_state: int,\n",
    "        use_input_proj_bias: bool = False,\n",
    "        use_delta_proj_bias: bool = False,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        self.d_inner = d_inner\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        (\n",
    "            key,\n",
    "            input_proj_key,\n",
    "            delta_proj_key,\n",
    "        ) = jax.random.split(key, 3)\n",
    "        self.input_proj = eqx.nn.Linear(\n",
    "            d_inner,\n",
    "            dt_rank + d_state * 2,\n",
    "            use_bias=use_input_proj_bias,\n",
    "            key=input_proj_key,\n",
    "        )\n",
    "\n",
    "        self.delta_proj = eqx.nn.Linear(\n",
    "            dt_rank, d_inner, use_bias=use_delta_proj_bias, key=delta_proj_key\n",
    "        )\n",
    "        A = (\n",
    "            jnp.repeat(jnp.arange(1, d_state + 1), d_inner)\n",
    "            .reshape(d_state, d_inner)\n",
    "            .transpose()\n",
    "        )\n",
    "        self.A_log = jnp.log(A)\n",
    "        self.D = jnp.ones(d_inner)\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"seq_length d_inner\"]):\n",
    "        A = -jnp.exp(self.A_log)\n",
    "        D = self.D\n",
    "\n",
    "        delta_b_c = jax.vmap(self.input_proj)(x)\n",
    "\n",
    "        split_indices = [\n",
    "            self.dt_rank,\n",
    "            self.dt_rank + self.d_state,\n",
    "        ]\n",
    "        delta, B, C = jnp.split(delta_b_c, split_indices, axis=-1)\n",
    "        delta = jax.nn.softplus(jax.vmap(self.delta_proj)(delta))\n",
    "\n",
    "        y = selective_scan(x, delta, A, B, C, D)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48addc-b78f-429b-9299-d1e283f3bd76",
   "metadata": {},
   "source": [
    "Armed with the `SSM`, we can now implement the `MambaBlock` part. See the images above for where we are right now! Thankfully, Equinox also includes the implementation above, which we can access from `eqx.nn.SelectiveStateSpaceModel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b57d346-18d0-4d29-b35d-e1a6e99c6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(eqx.Module):\n",
    "    in_proj: eqx.nn.Linear\n",
    "    conv1d: eqx.nn.Conv1d\n",
    "    ssm: SelectiveStateSpaceModel\n",
    "    out_proj: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd: int,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_conv: int,\n",
    "        use_in_projection_bias: bool = True,\n",
    "        use_conv_bias: bool = True,\n",
    "        use_out_proj_bias: bool = True,\n",
    "        ssm_use_delta_proj_bias: bool = False,\n",
    "        ssm_use_input_proj_bias: bool = False,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        (\n",
    "            key,\n",
    "            linear_key,\n",
    "            conv1d_key,\n",
    "            ssm_key,\n",
    "            out_proj_key,\n",
    "        ) = jax.random.split(key, 5)\n",
    "\n",
    "        self.in_proj = eqx.nn.Linear(\n",
    "            n_embd,\n",
    "            d_inner * 2,\n",
    "            use_bias=use_in_projection_bias,\n",
    "            key=linear_key,\n",
    "        )\n",
    "\n",
    "        self.conv1d = eqx.nn.Conv1d(\n",
    "            in_channels=d_inner,\n",
    "            out_channels=d_inner,\n",
    "            kernel_size=d_conv,\n",
    "            use_bias=use_conv_bias,\n",
    "            groups=d_inner,\n",
    "            padding=d_conv - 1,\n",
    "            key=conv1d_key,\n",
    "        )\n",
    "        self.ssm = SelectiveStateSpaceModel(\n",
    "            d_inner=d_inner,\n",
    "            dt_rank=dt_rank,\n",
    "            d_state=d_inner,\n",
    "            use_delta_proj_bias=ssm_use_delta_proj_bias,\n",
    "            use_input_proj_bias=ssm_use_input_proj_bias,\n",
    "            key=ssm_key,\n",
    "        )\n",
    "        self.out_proj = eqx.nn.Linear(\n",
    "            d_inner,\n",
    "            n_embd,\n",
    "            use_bias=use_out_proj_bias,\n",
    "            key=out_proj_key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: Array):\n",
    "        seq_len, d = x.shape\n",
    "        x_and_res = jax.vmap(self.in_proj)(x)\n",
    "\n",
    "        (x, res) = jnp.split(x_and_res, 2, axis=-1)\n",
    "        x = jnp.transpose(x)\n",
    "        x = self.conv1d(x)[:, :seq_len]\n",
    "        x = jnp.transpose(x)\n",
    "        x = jax.nn.silu(x)\n",
    "\n",
    "        y = self.ssm(x)\n",
    "        y = y * jax.nn.silu(res)\n",
    "\n",
    "        output = jax.vmap(self.out_proj)(y)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7543f-5bc9-480e-bab4-7f839cc8faad",
   "metadata": {},
   "source": [
    "Now, we can wrap the `MambaBlock` into the `ResidualBlock` -- as the name suggests, this has a residual connection (or in non-_sciency_ words: it adds the original input to the transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ed52a9-abea-43e8-822a-9d75ac8ae480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(eqx.Module, strict=True):\n",
    "    mamba_block: MambaBlock\n",
    "    rns_norm: eqx.nn.RMSNorm\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd: int,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_conv: int,\n",
    "        use_in_projection_bias: bool = True,\n",
    "        use_conv_bias: bool = True,\n",
    "        use_out_proj_bias: bool = True,\n",
    "        ssm_use_delta_proj_bias: bool = False,\n",
    "        ssm_use_input_proj_bias: bool = False,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        self.mamba_block = MambaBlock(\n",
    "            n_embd=n_embd,\n",
    "            d_inner=d_inner,\n",
    "            dt_rank=dt_rank,\n",
    "            d_conv=d_conv,\n",
    "            use_in_projection_bias=use_in_projection_bias,\n",
    "            use_conv_bias=use_conv_bias,\n",
    "            use_out_proj_bias=use_out_proj_bias,\n",
    "            ssm_use_delta_proj_bias=ssm_use_delta_proj_bias,\n",
    "            ssm_use_input_proj_bias=ssm_use_input_proj_bias,\n",
    "            key=key,\n",
    "        )\n",
    "        self.rns_norm = eqx.nn.RMSNorm(n_embd)\n",
    "\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"seq_len n_embd\"], *, key: Optional[PRNGKeyArray] = None\n",
    "    ) -> Array:\n",
    "        return self.mamba_block(jax.vmap(self.rns_norm)(x)) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8ed93-4946-4758-a228-cc39326b8c9d",
   "metadata": {},
   "source": [
    "We've arrived at the highest point again. We can put everything into the `Mamba` class now. Note that the weights of the embedding layer and the final linear layer are shared! This is not a problem though, because we can use `eqx.nn.Shared` to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91463596-114f-45f7-a2f0-b1ae8d16f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(eqx.Module, strict=True):\n",
    "    layers: eqx.nn.Sequential\n",
    "    normalization: eqx.nn.RMSNorm\n",
    "    shared_emb_lm_head: eqx.nn.Shared\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int,\n",
    "        n_dims: int,\n",
    "        n_embd: int,\n",
    "        d_inner: int,\n",
    "        dt_rank: int,\n",
    "        d_conv: int,\n",
    "        use_in_projection_bias: bool = True,\n",
    "        use_conv_bias: bool = True,\n",
    "        use_out_proj_bias: bool = True,\n",
    "        ssm_use_delta_proj_bias: bool = False,\n",
    "        ssm_use_input_proj_bias: bool = False,\n",
    "        *,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        key, *subkeys = jax.random.split(key, 3 + n_layers)\n",
    "        self.layers = eqx.nn.Sequential(\n",
    "            [\n",
    "                ResidualBlock(\n",
    "                    n_embd=n_embd,\n",
    "                    d_inner=d_inner,\n",
    "                    dt_rank=dt_rank,\n",
    "                    d_conv=d_conv,\n",
    "                    use_in_projection_bias=use_in_projection_bias,\n",
    "                    use_conv_bias=use_conv_bias,\n",
    "                    use_out_proj_bias=use_out_proj_bias,\n",
    "                    ssm_use_delta_proj_bias=ssm_use_delta_proj_bias,\n",
    "                    ssm_use_input_proj_bias=ssm_use_input_proj_bias,\n",
    "                    key=subkeys[i],\n",
    "                )\n",
    "                for i in range(n_layers)\n",
    "            ],\n",
    "        )\n",
    "        self.normalization = eqx.nn.RMSNorm(n_embd)\n",
    "\n",
    "        embedding = eqx.nn.Embedding(n_dims, n_embd, key=subkeys[n_layers])\n",
    "        lm_head = eqx.nn.Linear(\n",
    "            n_embd,\n",
    "            n_dims,\n",
    "            use_bias=False,\n",
    "            key=subkeys[n_layers + 1],\n",
    "        )\n",
    "        where = lambda embed_and_lin: embed_and_lin[1].weight\n",
    "        get = lambda embed_and_lin: embed_and_lin[0].weight\n",
    "        self.shared_emb_lm_head = eqx.nn.Shared(\n",
    "            (embedding, lm_head), where=where, get=get\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: Int[Array, \"seq_len\"],  # noqa\n",
    "        *,\n",
    "        key: Optional[PRNGKeyArray] = None,\n",
    "    ) -> Float[Array, \"seq_len n_dims\"]:  # noqa\n",
    "        embedding, linear = self.shared_emb_lm_head()\n",
    "        x = jax.vmap(embedding)(x)\n",
    "\n",
    "        x = self.layers(x)\n",
    "        x = jax.vmap(self.normalization)(x)\n",
    "        logits = jax.vmap(linear)(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557fa4a-1fa4-4e72-ba77-d0ef7f946cdf",
   "metadata": {},
   "source": [
    "Note the usage of `eqx.nn.Shared`:\n",
    "\n",
    "```python\n",
    "    # Embedding layer\n",
    "    embedding = eqx.nn.Embedding(\n",
    "        n_dims, n_embd, key=subkeys[n_layers]\n",
    "    )\n",
    "    # Linear layer\n",
    "    lm_head = eqx.nn.Linear(\n",
    "        n_embd,\n",
    "        n_dims,\n",
    "        use_bias=False,\n",
    "        key=subkeys[-1],\n",
    "    )\n",
    "    # refers to the linear weights\n",
    "    where = lambda embed_and_lin: embed_and_lin[1].weight \n",
    "\n",
    "    # refers to the embedding weights\n",
    "    get = lambda embed_and_lin: embed_and_lin[0].weight\n",
    "\n",
    "    # Create a shared layer\n",
    "    self.shared_emb_lm_head = eqx.nn.Shared(\n",
    "        (embedding, lm_head), where=where, get=get\n",
    "    )\n",
    "```\n",
    "\n",
    "And to use the shared layers, we have to get them first out of the shared layer:\n",
    "\n",
    "```python\n",
    "    embedding, linear = self.shared_emb_lm_head()\n",
    "    # embedding and linear are eqx.nn.Embedding and eqx.nn.Linear respectively\n",
    "    # proceed usage as usual\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07491e-70ea-43b3-9139-0940caf1dd50",
   "metadata": {},
   "source": [
    "Excellent! We have successfully implemented the Mamba model! From here we can train the model for example on the [TinyShakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset. We can use [Jaxonloader](https://github.com/Artur-Galstyan/jaxonloader), which provides the necessary preprocessing steps for us, so we can start training! Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24888c56-1c00-48f4-9e83-c7eaf5ce7b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m2 packages\u001b[0m in 16ms\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arturgalstyan/.pyenv/versions/3.11.8/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "!uv pip install jaxonloader optax\n",
    "# or simply\n",
    "# !pip install jaxonloader optax\n",
    "# if you don't use uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6edf447f-99cb-4335-829b-bfe293296b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "import math\n",
    "from collections.abc import Callable\n",
    "\n",
    "import optax\n",
    "from jaxonloader import (\n",
    "    get_tiny_shakespeare,\n",
    "    Index,\n",
    "    JaxonDataLoader,\n",
    "    JITJaxonDataLoader,\n",
    "    make,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d760872d-8f4d-4522-90d4-591e88e4b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, vocab_size, encode, decode = get_tiny_shakespeare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de3d0a4-601e-4ee6-a314-ee4c3e66a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_dataloader: JaxonDataLoader | JITJaxonDataLoader,\n",
    "    train_index: Index,\n",
    "    learning_rate: float,\n",
    "    model: PyTree,\n",
    "    key: PRNGKeyArray,\n",
    "    early_stop: int | None = None,\n",
    "    log_every: Optional[int] = 100,\n",
    ") -> PyTree:\n",
    "    optimizer = optax.adamw(learning_rate=learning_rate)\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    loss_value = 0\n",
    "    i = 0\n",
    "    while it := train_dataloader(train_index):\n",
    "        x, train_index, done = it\n",
    "        if done:\n",
    "            break\n",
    "        x, y = jnp.split(x, 2, axis=1)\n",
    "        key, subkey = jax.random.split(key)\n",
    "        model, opt_state, loss_value = step(\n",
    "            model, opt_state, x, y, optimizer, key=subkey\n",
    "        )\n",
    "        if log_every is not None and i % log_every == 0:\n",
    "            print(f\"Loss: {loss_value}\")\n",
    "        if early_stop is not None and i > early_stop:\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "    print(\"Finished training\")\n",
    "    print(f\"Final loss: {loss_value}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def loss_fn(\n",
    "    model: PyTree,\n",
    "    x: Int[Array, \"batch_size max_seq_len n_dims\"],\n",
    "    labels: Int[Array, \"batch_size max_seq_len n_dims\"],\n",
    "    key: Optional[PRNGKeyArray],\n",
    ") -> Array:\n",
    "    partial_model = ft.partial(model, key=key)\n",
    "    logits = eqx.filter_vmap(partial_model)(x)\n",
    "    return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, labels))\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def step(\n",
    "    model: PyTree,\n",
    "    opt_state: PyTree,\n",
    "    x: Array,\n",
    "    y: Array,\n",
    "    optimizer: optax.GradientTransformation,\n",
    "    key: PRNGKeyArray,\n",
    ") -> tuple[PyTree, PyTree, Any]:\n",
    "    loss, grads = eqx.filter_value_and_grad(loss_fn)(model, x, y, key)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "\n",
    "    return model, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae127d75-45d1-47b1-899c-6190632faf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, index = make(train_dataset, batch_size=64, jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992acff9-5eb4-4aa3-ba2b-8b13bddd4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mamba(\n",
    "    train_dataloader,\n",
    "    train_index,\n",
    "    n_dims: int,\n",
    "    n_embd: int,\n",
    "    expand: int,\n",
    "    d_state: int,\n",
    "    n_layers: int,\n",
    "    d_conv: int,\n",
    "    learning_rate: float,\n",
    "    early_stop: int,\n",
    "    key: PRNGKeyArray,\n",
    "):\n",
    "    mamba = Mamba(\n",
    "        n_layers=n_layers,\n",
    "        n_dims=n_dims,\n",
    "        n_embd=n_embd,\n",
    "        d_inner=int(expand * n_embd),\n",
    "        dt_rank=math.ceil(n_embd / d_state),\n",
    "        d_conv=d_conv,\n",
    "        key=key,\n",
    "    )\n",
    "    key, subkey = jax.random.split(key)\n",
    "    mamba = train(\n",
    "        train_dataloader,\n",
    "        train_index,\n",
    "        learning_rate,\n",
    "        mamba,\n",
    "        subkey,\n",
    "        early_stop=early_stop,\n",
    "    )\n",
    "    return mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88679246-7eff-491b-a115-64be758318f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = vocab_size\n",
    "n_embd = 64\n",
    "learning_rate = 3e-4\n",
    "num_heads = 4\n",
    "n_layers = 3\n",
    "d_state = 16\n",
    "d_conv = 4\n",
    "expand = 2\n",
    "early_stop = 1000\n",
    "key = jax.random.PRNGKey(222)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445edf0c-e48b-477b-a890-48ac6118fe85",
   "metadata": {},
   "source": [
    "You probably won't get very far using those parameters above and you'll have to increase these numbers to achieve greater performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592458f1-9680-4cb5-9173-3bf5452d58bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 59.77830123901367\n",
      "Loss: 4.061922073364258\n",
      "Loss: 2.9172539710998535\n",
      "Loss: 2.7635631561279297\n",
      "Loss: 2.735884189605713\n",
      "Loss: 2.3609986305236816\n",
      "Loss: 2.490962505340576\n",
      "Loss: 2.332058906555176\n",
      "Loss: 2.2114100456237793\n",
      "Loss: 2.3747291564941406\n",
      "Loss: 2.366851329803467\n",
      "Finished training\n",
      "Final loss: 2.29056978225708\n"
     ]
    }
   ],
   "source": [
    "mamba = train_mamba(\n",
    "    train_dataloader,\n",
    "    index,\n",
    "    n_dims,\n",
    "    n_embd,\n",
    "    expand,\n",
    "    d_state,\n",
    "    n_layers,\n",
    "    d_conv,\n",
    "    learning_rate,\n",
    "    early_stop,\n",
    "    key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6a9732-7d0e-4593-8a5d-46a26b3f5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: PyTree,\n",
    "    max_seq_len: int,\n",
    "    max_new_tokens: int,\n",
    "    decode: Callable,\n",
    "    vocab_size: int,\n",
    "    print_to_console: bool = True,\n",
    "    random_key_seed: int = 0,\n",
    ") -> tuple[list[str], list[int]]:\n",
    "    jitted_model = eqx.filter_jit(model)\n",
    "    x = jnp.zeros((max_seq_len,), dtype=jnp.int32)\n",
    "    key = jax.random.PRNGKey(random_key_seed)\n",
    "    tokens = []\n",
    "    decoded_tokens = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        key, subkey, model_key = jax.random.split(key, 3)\n",
    "        logits = jitted_model(x, key=model_key)\n",
    "        logits = logits[-1, :]\n",
    "        probs = jax.nn.softmax(logits, axis=-1)\n",
    "\n",
    "        next_token = jax.random.choice(\n",
    "            subkey,\n",
    "            jnp.arange(len(probs)),\n",
    "            p=probs,\n",
    "        )\n",
    "        next_token = jnp.array(next_token, dtype=jnp.int32).reshape((1,))\n",
    "        next_token = min(next_token.item(), vocab_size - 1)\n",
    "\n",
    "        if print_to_console:\n",
    "            print(decode([next_token]), end=\"\")\n",
    "\n",
    "        tokens.append(next_token)\n",
    "        decoded_tokens.append(decode([next_token]))\n",
    "\n",
    "        x = jnp.concatenate((x[1:], jnp.array([next_token])))\n",
    "    return decoded_tokens, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60c579b-d18d-49c8-b87f-2b24daf5d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OfRT:\n",
      "Thet buce,\n",
      "Whaduti fot se w,\n",
      "LI:\n",
      "An\n",
      "Ber?\n",
      "\n",
      "NOLIOLONULLEO:\n",
      "Os to myse, fFieln torm's\n",
      "Durme mence th thates myt hit bere hou ps prore onoth but to come beat inwas notven bestot bucn mend not thats "
     ]
    }
   ],
   "source": [
    "text = generate_text(mamba, 8, 200, decode, vocab_size)  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db62c07-d466-4fcd-a7d1-031cb300ed78",
   "metadata": {},
   "source": [
    "Truly, a magnificient masterpiece. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
